{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2b00624-8e86-4bb2-888b-e63c305b125e",
   "metadata": {},
   "source": [
    "# Bear 2 - Introduction to Model Extraction\n",
    "\n",
    "## Challenge Description\n",
    "\n",
    "*You've stumbled upon an API endpoint that returns the measurements of a bear's mood (specifically happiness) from an input value of the bear's hunger (a value between 0 and 100).*\n",
    "\n",
    "*Your goal is to perform model extraction on this model to understand the internal relationship between these features in the model. Once you've determined the coefficient value associated with a bear's hunger and happiness you can submit to the api endpoint to recieve the flag.*\n",
    "\n",
    "## What is Model Extraction?\n",
    "\n",
    "Model extraction is a technique used to infer the properties, structure, or parameters of a machine learning model based solely on its outputs. This process can reveal insights about how input variables are transformed into predictions, allowing for the replication or analysis of the model without direct access to its internal workings. It plays a crucial role in understanding proprietary, black-box models, ensuring transparency and fairness in machine learning applications.\n",
    "\n",
    "## Challenge Setup\n",
    "\n",
    "You are provided with access to a machine learning model through an API endpoint. This endpoint accepts a parameter named Hunger, representing the hunger level of a bear on a scale from 0 to 100 (inclusive), and returns the bear's Happiness level as a response. Although this scenario is simplified, it presents a practical introduction to the strategies and considerations involved in model extraction.\n",
    "\n",
    "## Below are the steps we will walk through to solve this challenge\n",
    "\n",
    "1. `Explore the Endpoint`: Begin by making initial requests to the API with varied Hunger values. Note the structure of the responses and any patterns or anomalies in the Happiness outputs. This step helps you understand the range and type of data you'll be working with.\n",
    "\n",
    "2. `Data Collection`: Systematically collect data points by sending requests across the entire range of Hunger values (0-100). Consider automating this process with a script that iterates through possible Hunger values, capturing the corresponding Happiness levels. This dataset will be foundational for your analysis.\n",
    "\n",
    "3. `Analysis`: With your collected data, analyze the relationship between Hunger and Happiness. Look for trends, such as linear relationships, curvatures, or discrete jumps, that can give clues about the underlying model.\n",
    "\n",
    "4. `Model Replication`: Based on your analysis, attempt to replicate the model's behavior. This could involve fitting a regression model, using machine learning algorithms, or applying mathematical theories to approximate the discovered relationship.\n",
    "\n",
    "5. `Extraction and Reporting`: Finally, extract the coefficient that quantifies the relationship between Hunger and Happiness. Submit this to the endpoint to recieve the flag!\n",
    "\n",
    "In this challenge we are given access to a machine learning model through an API endpoint. The endpoint takes in the `Hunger` value of a bear (values between 1-100) and return the `Happiness` of the bear. This is a simple example but will help us understand the techniques we can use to extract data.\n",
    "\n",
    "Lets start by testing the API endpoint and seeing what types of outputs we get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d06992f-6ee9-46cc-81e2-df7041bf1075",
   "metadata": {},
   "outputs": [],
   "source": [
    "CRUCIBLE_API_KEY = \"YOUR_API_KEY\"  # Replace with your actual API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63fe0c9-efc6-4ac4-a252-3ea5b0de46b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import requests\n",
    "\n",
    "input_data = {\"hunger\": 5}\n",
    "\n",
    "\n",
    "def query(input_data):\n",
    "    payload = {\"data\": input_data}\n",
    "    headers = {\"Authorization\": CRUCIBLE_API_KEY}\n",
    "    response = requests.post(\n",
    "        \"https://bear2.crucible.dreadnode.io/score\",\n",
    "        headers=headers,\n",
    "        json=input_data\n",
    "    )\n",
    "\n",
    "    return response.json()\n",
    "\n",
    "\n",
    "response = query(input_data)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19b95e2-1314-4a4f-8705-3a0e664486a3",
   "metadata": {},
   "source": [
    "## Step 1: Explore the Endpoint\n",
    "\n",
    "Lets start by trying different requests to the endpoint and seeing what we get back. It's best not to make any assumptions at this point to see if there are any potential approaches to extracting more information about the model.\n",
    "\n",
    "We will use the `query` function provided to access the endpoint. **Note** you must replace the API_KEY with your own key for this challenge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f8d161-0ea6-4e6e-9d49-d70cf156c1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "query({\"hunger\": 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea29457-26ff-4d83-ab6f-7cf8bf84918a",
   "metadata": {},
   "outputs": [],
   "source": [
    "query({\"hunger\": 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fd95d3-733a-46d5-a90e-1b2bc4b39738",
   "metadata": {},
   "outputs": [],
   "source": [
    "query({\"hunger\": -5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7756556-9908-4d5d-82e6-938f1c0a922f",
   "metadata": {},
   "outputs": [],
   "source": [
    "query({\"hunger\": 999})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c9080b-772a-4b92-9a47-3b8b8157aa13",
   "metadata": {},
   "outputs": [],
   "source": [
    "query({\"hunger\": 'test'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a83c96-b962-40ef-b0ea-8658587aeb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "query({\"hunger\": 50.00000000000000000000001})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b20b9d-8a16-4b14-9fd5-eadb1579a4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "query({\"coef\": 0.1})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6805f32f-567c-46c3-ad2a-89e86848cd2f",
   "metadata": {},
   "source": [
    "## Step 2: Data Collection\n",
    "\n",
    "In this phase, we systematically explore the relationship between the bear's hunger and its happiness by querying the API across the entire range of hunger values, from 0 to 100. This comprehensive data collection is essential for accurately modeling the underlying relationship.\n",
    "\n",
    "To ensure we capture every possible value, our loop will include the upper boundary by iterating from 0 to 100, inclusive. Automating this process with a script not only saves time but also guarantees the precision of our data collection effort.\n",
    "\n",
    "Here's how we can automate the data collection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb8a472-4c11-4092-ab5d-ad4565a3a9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {} # Store the results from the endpoint here\n",
    "\n",
    "for h in range(101):\n",
    "    response = query({\"hunger\": h})\n",
    "    results[h] = response['outputs'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bab5767-2e83-4eb3-b9c0-43bc781788f9",
   "metadata": {},
   "source": [
    "This step is crucial for building a dataset that represents the full spectrum of the model's input-output relationship. With this data in hand, we can begin to analyze how changes in hunger affect the bear's happiness, setting the stage for accurately replicating and eventually extracting the model's internal mechanics.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cdb5e00-7129-4082-b9ad-24ec901b0708",
   "metadata": {},
   "source": [
    "# Step 3: Analysis\n",
    "\n",
    "With a comprehensive dataset of hunger and happiness values now at our disposal, the next step is to analyze these data to discern patterns, trends, or relationships that could hint at the underlying model the API uses. This analysis is important in understanding how changes in hunger levels influence happiness scores, thereby revealing insights into the model's structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e018e8-6a1e-4b28-b4d3-989e1075d6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # For data analysis\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.DataFrame(list(results.items()), columns=['Hunger', 'Happiness'])\n",
    "\n",
    "# Plotting Hunger vs. Happiness\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(df['Hunger'], df['Happiness'], alpha=0.6)\n",
    "plt.title('Hunger vs. Happiness Analysis')\n",
    "plt.xlabel('Hunger Level')\n",
    "plt.ylabel('Happiness Score')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f58c64-43f2-4f91-a4d7-a9968503dd38",
   "metadata": {},
   "source": [
    "A scatter plot is particularly useful for this type of analysis as it allows you to visually inspect the relationship between the two variables.\n",
    "\n",
    "1. `Trend Identification`: After plotting, look for any apparent trends. A linear trend might suggest a simple linear relationship, whereas a curve could indicate a polynomial relationship. Discrete jumps or plateaus might imply conditional logic within the model.\n",
    "\n",
    "2. `Hypothesis Formulation`: Based on your observations, formulate hypotheses about the model's behavior. For instance, if the plot shows a linear relationship, one might hypothesize that the model uses a linear equation to relate hunger to happiness.\n",
    "\n",
    "3. `Preliminary Conclusions`: Draw preliminary conclusions about the potential model structure. These insights will guide the replication efforts in the next step.\n",
    "\n",
    "Some other things to consider:\n",
    "\n",
    "1. `Linearity`: If the relationship appears linear, the model might be using a simple linear regression. The slope of the line could directly indicate the coefficient we're seeking.\n",
    "2. `Non-Linearity`: For non-linear relationships, consider more complex models or transformations. Polynomial regression or non-linear models may be required.\n",
    "3. `Anomalies`: Keep an eye out for any outliers or anomalies in the data that could indicate exceptions in the model's logic.\n",
    "\n",
    "\n",
    "This analytical step is foundational, setting the stage for accurately modeling the observed relationship in the subsequent steps. By understanding the nature of the data, we can tailor our model replication efforts to mirror the API's behavior as closely as possible.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06bbe9c9-d1e4-4df3-a3b1-7821b3a08b22",
   "metadata": {},
   "source": [
    "# Step 4. Model Replication\n",
    "\n",
    "After analyzing the data and identifying potential relationships between hunger and happiness, the next step is to attempt replicating the model's behavior. This involves using the insights gained from your analysis to construct a model that closely mimics the API's output patterns. The goal of model replication is to understand and replicate the decision-making process of the original model as closely as possible.\n",
    "\n",
    "Choosing the Right Model:\n",
    "\n",
    "Based on the trends and patterns identified in the previous step, decide on the type of model that best fits the observed data. For a linear relationship, a simple linear regression model may suffice. If the relationship appears more complex, consider using polynomial regression or another suitable model.\n",
    "\n",
    "Model Replication Process:\n",
    "\n",
    "1. `Prepare the Data`: Convert your collected data into a format suitable for model training. If you're using a linear regression model, your existing dataset format should be adequate.\n",
    "\n",
    "2. `Select a Modeling Tool`: Choose a tool or library that fits your chosen model type. For linear and polynomial regression, libraries like scikit-learn in Python are ideal due to their simplicity and extensive documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfcb6fcd-6cdd-4215-b621-9c9e75f40a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import  LinearRegression\n",
    "\n",
    "lr = LinearRegression() # Create a simple linear regression model\n",
    "\n",
    "lr.fit(df[['Hunger']], df['Happiness'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc15b941-e78f-451e-a54d-69e0cad40189",
   "metadata": {},
   "source": [
    "### Model Validation:\n",
    "\n",
    "Validate your replicated model by predicting happiness levels for a new set of hunger values not used in the training phase. This step ensures that your model can generalize well and accurately reflects the original model's behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ec582b-dd2c-4012-8064-db261d8a23c3",
   "metadata": {},
   "source": [
    "## Step 5. Extraction and Reporting\n",
    "\n",
    "Having successfully replicated the model in Step 4, the final task involves extracting the key insights, specifically the coefficient(s) that define the relationship between a bear's hunger and its happiness. This step is crucial for understanding the underlying mechanics of the model and for fulfilling the challenge's objective by submitting the extracted coefficient to receive the flag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af533250-f3bf-4abc-90ba-a8d2b35ae374",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d85c364-1916-4d5b-9c5b-8c123a6af368",
   "metadata": {},
   "source": [
    "## Submit and recieve flag\n",
    "\n",
    "Lets submit the coef and see if it gets us to result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd210dde-bdc7-482d-bb10-37ced01f35a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_data = {\"coef\": \"*PUT COEF VALUE HERE TO GET THE FLAG*\"}\n",
    "\n",
    "query(coef_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38c6a44-b2e8-406f-a945-5167e3e17f05",
   "metadata": {},
   "source": [
    "# Submit The Flag\n",
    "\n",
    "Once you have the flag you can use the following function to submit the flag to Crucible. Alternatively you can submit it direction on the Crucible website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e49dd91-ed67-491c-81db-45f190d9fc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def submit_flag(flag, challenge=\"bear2\"):\n",
    "    url = \"https://crucible.dreadnode.io/api/submit-flag\"\n",
    "    headers = {\"Authorization\": CRUCIBLE_API_KEY}\n",
    "    payload = {\"challenge\": challenge, \"flag\": flag}\n",
    "    response = requests.post(url, headers=headers, json=payload)\n",
    "    if response.status_code == 200:\n",
    "        if response.json().get(\"correct\") is True:\n",
    "            print(\"The flag was correct. Congrats!\")\n",
    "        else:\n",
    "            print(\"The flag was incorrect. Keep trying!\")\n",
    "    else:\n",
    "        print(\"There was an error submitting your flag\")\n",
    "        print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a968483d-878f-45d5-b724-5b942b52e916",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_flag()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52fe4b05-667d-4c94-9170-9e6a83dca28f",
   "metadata": {},
   "source": [
    "## Future considerations\n",
    "While the challenge you've just navigated provides a foundational understanding of model extraction, it represents only the tip of the iceberg. The complexity and nuances of real-world machine learning models demand a broader set of strategies and considerations. As you advance to more sophisticated challenges, several key areas will play a pivotal role in your ability to accurately extract and replicate models:\n",
    "\n",
    "- **Data Preprocessing**: The quality of your data directly impacts the effectiveness of model extraction. Techniques such as normalization, handling missing values, and feature engineering are crucial for preparing data that accurately reflects real-world conditions. Mastering these preprocessing steps ensures that the data feeding into your models is clean, relevant, and structured in a way that maximizes the model's performance.\n",
    "- **Model Regularization**: To prevent overfitting and enhance the generalizability of your models, regularization techniques become indispensable. Regularization methods, such as L1 and L2 regularization, help in creating models that are robust and perform well on unseen data, not just the data they were trained on. Understanding when and how to apply these techniques will be critical as you encounter models with a high risk of overfitting due to complexity or limited training data.\n",
    "- **Handling Random Noise**: Real-world data is often noisy and imperfect. Learning to differentiate between signal and noise is crucial for model extraction efforts. Techniques for filtering out noise and focusing on the underlying patterns in the data are essential tools in your arsenal, especially as you deal with datasets that include random variations or errors.\n",
    "- **Dealing with Complex Models**: As models increase in complexity, from deep neural networks to ensemble methods, the strategies for model extraction must also evolve. You'll need to adapt your approach to handle multiple layers of abstraction, non-linear relationships, and models that rely on a vast number of parameters. This may involve developing intuition for how different types of models process information and learning specialized techniques for extracting insights from specific model architectures.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4cfbb5-dcfb-4ca8-8514-1485082a8546",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Engaging in model extraction challenges offers a unique lens through which to understand and interact with machine learning models. This journey not only sharpens your analytical and problem-solving skills but also deepens your appreciation for the complexity inherent in these models. Through practical experience, you gain insights into how models interpret data and make predictions, equipping you with a versatile toolkit for tackling real-world data science challenges. Such endeavors underscore the importance of critical thinking and adaptability, foundational qualities for success in the ever-evolving landscape of machine learning and data science.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
